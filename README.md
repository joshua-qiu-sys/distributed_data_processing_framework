# Distributed Data Processing Framework
The distributed data processing framework is built upon a Lambda architecture that combines both the batch processing capabilities of Pyspark and the event streaming capabilities of Kafka. It provides a consistent, configuration-driven approach for data ingestion, transformation and distribution and enables highly scalable pipelines to be developed as the variety of data sources/sinks increases or the volume of data increases. Common utilities are also included as part of the framework which support commonly used operations across the data pipeline stages including connectors to data sources/sinks, data validation checkers, application loggers, etc.